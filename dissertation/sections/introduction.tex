\documentclass[../report.tex]{subfiles}

\begin{document}

\section{INTRODUCTION}

As the prevalence of smartphones and internet of things (IoT) devices increasingly dictate the human experience, the software industry has pivoted connectivity-centered development. A common mobile software application is fully reliant on an internet connection, acting almost exclusively as a recipient for transmitted data. These clients typically serve purposes such as social media communication, content consumption, user account access etc.

This reliance on connectivity means the software becomes a front-end client in a distributed computing system, with the logistical and computational responsibilities assigned and distributed to and among a network of back-end servers. With this reality comes a series of architectural considerations and decisions that inform how and to what extent the front-end software is developed.

A main feature of distributed systems is their ability to handle hardware and software heterogeneity, as information must travel digitally and physically across layers of applications, networks, and hardware. Another feature is their ability to transmit and parse information across layers despite the differences in protocols, programming language features, and data formats. As data transmission has become ubiquitous, . Section \ref{sec:vocabulary} explores the landscape of distribution and data transmission in-depth, to establish a vocabulary of terminology.

Existing research predominantly evaluates data serialisation formats from a feature, performance, and efficiency perspective. While these aspects are quantifiable, measurable, and potentially motivate decisions at large-scale data transmission they fail to illustrate the conditions that inform the choice of data format for the common software development team. These conditions are likely more abstract and extend beyond development into the organisational structures and division of ownership in the team or company.

The contribution of this paper is to evaluate the balance between readability and safety in the JSON data serialisation format, in order to propose an extensible format informed by the value perspective of developers. This proposed format, conceptualised as the Type-Extensible Object Notation (TXON), is paired with a translation layer written in JavaScript, to provide full compatibility with JSON data and existing language parsing.

To preface this proposal, it is important to consider the rich history and existing structures that this project builds upon.

% History of programming languages and data formats
%% XML and its impact on JSON typesetting

% Design philosophy in programming languages and data formats
%% Type-safe object-oriented programming
%% Extensibility in software development (language: Swift, data: [XML, TypeScript])

% Grounded Theory as a methodological approach
%% Personas and other related design theory

% OLD INTRODUCTION BELOW

As the internet increasingly dictates human life, connectivity and mobility becomes the focal point of personal technologies. This is evident from a consumer perspective but also in software development, where applications predominantly act as clients awaiting and receiving transmitted data from centralised and distributed systems. These computing systems consist of multiple bodies, often of heterogeneous types, which introduces the potential for conflicts in translation between programming languages and data formats. \\

Translation between bodies is accomplished through a process of object/data serialisation (and inversely deserialisation). Validation is typically performed after deserialisation, to align expectations with the received data, and to correct any conflicts before parsing. Heterogeneity indicates a inconsistency in architecture, language, data representation, instruction set and hardware capabilities. As a result, the data transmitted is serialised and deserialised to a homogeneous format shareable across heterogeneous bodies. \\

... \\

Existing research has already compared formats on syntax, performance, and efficiency, but has not addressed the architectural differences. As serialisation does not exist in a vacuum, the shortcomings of a format have to be guarded against through the development of defensive mechanisms. The popular programming language TypeScript inspired this project, as its design philosophy of extensible typesetting allows developers to improve their code without a larger implementation or system architectural changes.

The contribution of this paper is to provide an extensible syntax for typesetting in JSON, while maintaining full backwards compatibility. This is accomplished by abstracting the format to a translation layer during serialisation and parsing, to guard against validation conflicts and to reduce resources spent on debugging.

%%%

% As mobile and internet of things (IoT) devices continue to dominate the computing space, software development increasingly centers around clients receiving transmitted data. This system of connected devices is known as distributed computing, consisting of multiple heterogeneous or homogeneous bodies.

% Heterogeneity indicates a inconsistency in architecture, language, data representation, instruction set and hardware capabilities. As a result, the data transmitted is serialised and deserialised to a homogeneous format shareable across heterogeneous bodies. This dependence on serialisation motivates the comparison of different formats for object serialisation. 

% Existing research has already compared formats on syntax, performance, and efficiency, but has not addressed the architectural differences. As serialisation does not exist in a vacuum, the shortcomings of a format has to be guarded against through the development of defensive mechanisms.

% The contribution of this paper is to provide an architectural comparison of object serialisations formats. This is accomplished by comparing the formats themselves, then modelling software architectures for object serialisation.

%%%

% In the year 2021, ?\% of web traffic and ?\% of content consumption was attributed to mobile devices. The rise of the smartphone and "Internet of Things" (IOT) devices has shifted software markets and the attention of developers away from the desktop. In combination with the expansion of social media, most software now acts as content consumption clients; requesting and receiving serialised data through REST APIs, then parsing and presenting it to users. Serialisation of data is typically handled on the backend (server), while the frontend (application) handles deserialisation.

% The contribution of this paper is to provide the software development community a comparative assessment of the trade-off between usability and type safety between human-readable and binary data serialisation formats. The ubiquity of data transmission across heterogeneous languages, formats, and systems in a distributed computing environment, necessitates an evaluation of data serialisation formats for software implementation. Human-readable formats such as XML and JSON are comparatively analysed to binary formats such as Google's Protocol Buffers. Another alternative is a full stack solution that handles the entire stack of data transmission and serialisation, to guard against type errors etc.

\end{document}