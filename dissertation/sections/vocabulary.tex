

\documentclass[../report.tex]{subfiles}

\begin{document}

\section{VOCABULARY}

This section explores fundamental principles of the system architectures that support distribution of serialised data. It serves to establish a vocabulary for communication in computing, as well as to provide background knowledge on how data becomes distributed, and motivate my choices of systems.

\subsection{Distributed Computing}

\cite{kshemkalyani2011distributed} define \textit{distributed systems} as "a collection of independent entities that cooperate to solve a problem that cannot be individually solved." They characterise distributed computing as  "a collection of mostly autonomous processors communicating over a communication network". They identify common features of distributed systems, notably a lack of shared resources which necessitates communication, autonomy and heterogeneity. \\

In characterising distributed systems, \cite{kshemkalyani2011distributed} raise the notion that the physical differences of entities, and variation in their resources, creates a reliance on distributed communication. Distributed resources, particularly the absence of shared memory, implies an inherent asynchrony between entities. This means that each individual entity must act autonomously, while collaborating with and distributing tasks among the entities within the system. \\

As seen in figure \ref{fig:networkcomms}, a distributed system achieves asynchronous collaboration through a communication network, either a wide (WAN) area or local (LAN) area network, depending on the geography of the system. Each entity in the system consists of at least one processor (P) with its own solitary memory (M), providing the entity computational autonomy. This network structure creates the potential for both hardware and software heterogeneity, which necessitates coordination and distribution of tasks and responsibilities. \\

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\linewidth]{figures/networkcomms.pdf}
\caption{A distributed system connects processors by a communication network \cite[2]{kshemkalyani2011distributed}.}
\label{fig:networkcomms}
\end{figure}

\textit{Hardware heterogeneity} manifests as a variation in physical resources and thus implicitly a variation in computational capability. This necessitates a variance in entity responsibilities, as each individual entity is... This can of course be a cognitive decision made by system architects and engineers, facilitating a more efficient distribution, as computational tasks are inherently varied in requirements.

\textit{Software heterogeneity} manifests as a variation in programming languages and frameworks. Distributed systems use a layered architecture, with a middle layer driving the software distribution, the so-called "middleware." As seen in figure \ref{fig:processinteraction}, the middleware layer exists as an addition to the protocol-oriented application layer, which handles the communication protocols such as \textit{http}. Additionally, as data flows in a heterogeneous distributed system, it must adhere to a standardised and yet interoperable format, modelled on the software systems used in the network. \\

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\linewidth]{figures/processinteraction.pdf}
\caption{Interaction of the software components at each processor \cite[3]{kshemkalyani2011distributed}.}
\label{fig:processinteraction}
\end{figure}

At this point, you are probably wondering \textit{why distribution is relevant} to this project, and \textit{how it pertains to data serialisation.} \cite{kshemkalyani2011distributed} define various potential requirements of a system, that would motivate heterogeneous distribution. As data serialisation is typically utilised for distributed communication, it is not inherently performance- or scalability-oriented, but it is inherently distributed and geographically remote. The implication is that the inherent distributed nature of serialised data exchange necessitates the use of a distributed system, and not vice versa. \\

Given the inherent nature of serialised data exchange, and the ubiquity of distributed computing systems, the design of such systems informs our approach to communicating across them. It should be noted that systems do not exist in a vacuum, and thus system should be contextually designed based on market forces. System designers must balance or choose between industry standard protocols, which maximise interoperability, and the technically best solutions, which require more control and closed source development.

\begin{figure*}
\centering
\includegraphics[width=0.8\linewidth]{figures/pubsubsystem.pdf}
\caption{Components of a \textit{publish-subscribe} system. \cite[9]{tarkoma2012publish}.}
\label{fig:pubsubsystem}
\end{figure*}

\cite{kshemkalyani2011distributed} identify a set of design challenges applicable to the traditional server-client model of distributed systems. An \textit{Applications Programming Interface (API)} enables the distributed system to communicate internally and more importantly externally, maximising the adoption of system services by outside forces. It introduces the challenge of \textit{transparency}, as the system should be accessible without revealing its internal operations (resource [re]location, replication, concurrency, failure handling etc.) and implementation policies.

\cite{kshemkalyani2011distributed} describe several applications of distributed computing, of which the publish-subscribe model of content distribution is particularly relevant to this project, because it is the most prominent server-client system. In this model, information is filtered by relevancy, meaning the server distributes only the requested information. As argued by \cite{kshemkalyani2011distributed}, information distribution requires three types of mechanisms: distribution (publishing), specific requests (subscribe), and the ability to manipulate information based on a request before publishing. \\

In the following section on \textit{the transmission of data in distributed computing systems}, I present an in-depth illustration of how publish-subscribe models facilitate distribution of data-driven systems.

\subsection{Data Transmission}

This section explores the standards and methods for distributed communication through a publish-subscribe service. It serves as background knowledge on how serialised data is distributed, to illustrate how our choice of data serialisation library is informed by the system model. \\

\cite{tarkoma2012publish} defines \textit{publish-subscribe (pub/sub)} as the efficient and timely selective communication of events between participating components. He relates his conceptual perspective to how humans selectively focus on (or "subscribe" to) probable sources of interesting events.

He notes that participants in this type of distributed system would appear sourceless to each other, and thus they publish without direction. This introduces the crucial element of time beyond the typical asynchrony, as participants subscribe based on the probability that information will be communicated, even if no information yet exists. He contrasts this with database systems, wherein information is retrieved through queries, aimed at previously communicated information, rather than aimed at future communication. \\

\cite{tarkoma2012publish} illustrates the structural components of a pub/sub system through figure \ref{fig:pubsubsystem}, as well as how the participants interact through events and notifications. Publishers and subscribers are referred to as the \textit{main entities}, and publishers are the starting point for the chain of events in the system. As a situation occurs, referred to as an \textit{event}, the publisher detects it and publishes a notification to the service, also referred to as the \textit{event message}. Events denote \textit{discrete} measurable changes in the \textit{state} of a situation. The pub/sub service handles the communication infrastructure, and subscribers must express interest in a publisher before an event.

The nature of this relationship between publishers and subscribers introduces coordination challenges, as publishers and subscribers must agree on event expectations before a situation occurs. The pub/sub system does not take responsibility for these challenges, as it can only set expectations but not solve conflicts. The system is only responsible for delivering the communicated event between publishers and their subscribers. \\

The system can take different approaches to communicating events between entities over a network. In the simplest form, a pub/sub system can communicate directly from publishers to subscribers, with publishers taking on responsibility for the transmission of events. As the network scales and increasingly more entities subscribe this approach become untenable, and responsibilities are instead delegated to an intermediary type of entity, referred to as \textit{brokers} or \textit{pub/sub routers}.

In a \textit{centralised} pub/sub system, publishers either utilise a \textit{one-to-one message protocol}, or they communicate events to a \textit{broker server}, which forwards messages to its subscribers. In a \textit{distributed} pub/sub system events are never communicated directly between publishers and subscribers, and brokers are deployed as an \textit{overlay network} for routing. This overlay is an additional layer on-top of the network, allowing the brokers to collaboratively transmit events between entities, gaining the aforementioned advantages of a distributed system. \\

The network layer is one of multiple layers typical of networking systems. As seen in figure \ref{fig:ositcilayers}, \cite{alani2014guide} presents the 7 layers of the \textit{Open Systems Interconnection} (OSI) model relative to the 4 layers of the \textit{Transmission Control Protocol} (TCP). The OSI model abstracts networking systems into a conceptual framework, to describe and standardise the functional relationship between these layers. The TCI/IP model maps to the OSI model, but   \\

% The transport layer manages the delivery and error checking of data packets. It regulates the size, sequencing, and ultimately the transfer of data between systems and hosts. One of the most common examples of the transport layer is TCP or the Transmission Control Protocol.

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\linewidth]{figures/ositcilayers.pdf}
\caption{Comparison between layers in the OSI model and TCI/IP model, providing a standard communication architecture for distributed processing systems \cite[21]{alani2014guide}.}
\label{fig:ositcilayers}
\end{figure}

As data in a distributed systems flows from the software (\textit{application layer}) to the hardware (\textit{physical layer}), it is transformed by protocols which add additional information to the data. This process is referred to as \textit{encapsulation}, and consists of \textit{capsulation} from the source host and \textit{decapsulation} towards the destination host. As data flows from source host's application layer and towards the physical layer, protocols prepend headers (leading information) and append trailers (trailing information) to the data. This additional information indicates the purpose of communicating the data and how it should be interpreted by the next layer. \\

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\linewidth]{figures/encapsulation.pdf} \\
\vspace{0.5cm}
\includegraphics[width=0.8\linewidth]{figures/endtoendflow.pdf}
\caption{End-to-end data flow and encapsulation with headers and trailers \cite[15]{alani2014guide}.}
\label{fig:dataflow}
\end{figure}

\color{red}
This transformation of distributed data has consequences for...

The take away from the OSI model is that as data flows through a distributed system, it is transformed by protocols utilised in the layers. These protocols inform the state of the data...

REST state and APIs... \cite{tarkoma2012publish}
\color{black}

% The OSI Model (Open Systems Interconnection Model) is a conceptual framework used to describe the functions of a networking system. The OSI model characterises computing functions into a universal set of rules and requirements in order to support interoperability between different products and software. In the OSI reference model, the communications between a computing system are split into seven different abstraction layers: Physical, Data Link, Network, Transport, Session, Presentation, and Application.

% Created at a time when network computing was in its infancy, the OSI was published in 1984 by the International Organisation for Standardisation (ISO). Though it does not always map directly to specific systems, the OSI Model is still used today as a means to describe Network Architecture. \\

% \textbf{The 7 Layers of the OSI Model} \\

% \textbf{Physical Layer.} The lowest layer of the OSI Model is concerned with electrically or optically transmitting raw unstructured data bits across the network from the physical layer of the sending device to the physical layer of the receiving device. It can include specifications such as voltages, pin layout, cabling, and radio frequencies. At the physical layer, one might find “physical” resources such as network hubs, cabling, repeaters, network adapters or modems.

% \textbf{Data Link Layer.} At the data link layer, directly connected nodes are used to perform node-to-node data transfer where data is packaged into frames. The data link layer also corrects errors that may have occurred at the physical layer. The data link layer encompasses two sub-layers of its own. The first, media access control (MAC), provides flow control and multiplexing for device transmissions over a network. The second, the logical link control (LLC), provides flow and error control over the physical medium as well as identifies line protocols.

% \textbf{Network Layer.} The network layer is responsible for receiving frames from the data link layer, and delivering them to their intended destinations among based on the addresses contained inside the frame. The network layer finds the destination by using logical addresses, such as IP (internet protocol). At this layer, routers are a crucial component used to quite literally route information where it needs to go between networks.

% \textbf{Transport Layer.} The transport layer manages the delivery and error checking of data packets. It regulates the size, sequencing, and ultimately the transfer of data between systems and hosts. One of the most common examples of the transport layer is TCP or the Transmission Control Protocol.

% \textbf{Session Layer.} The session layer controls the conversations between different computers. A session or connection between machines is set up, managed, and termined at layer 5. Session layer services also include authentication and reconnections.

% \textbf{Presentation Layer.} The presentation layer formats or translates data for the application layer based on the syntax or semantics that the application accepts. Because of this, it at times also called the syntax layer. This layer can also handle the encryption and decryption required by the application layer.

% \textbf{Application Layer.} At this layer, both the end user and the application layer interact directly with the software application. This layer sees network services provided to end-user applications such as a web browser or Office 365. The application layer identifies communication partners, resource availability, and synchronises communication.

% Formats, Protocols, Blockchain (Git)?

\subsection{Object Serialisation} % why, state, formats, advantages and disadvantages

% \textbf{Serialisation} is the process of converting a data object—a combination of code and data represented within a region of data storage—into a series of bytes that saves the state of the object in an easily transmittable form. In this serialised form, the data can be delivered to another data store (such as an in-memory computing platform), application, or some other destination. The reverse process—constructing a data structure or object from a series of bytes—is \textbf{deserialisation}. The deserialisation process recreates the object, thus making the data easier to read and modify as a native structure in a programming language.

% Serialisation enables us to save the state of an object and recreate the object in a new location. serialisation encompasses both the storage of the object and exchange of data. Since objects are composed of several components, saving or delivering all the parts typically requires significant coding effort, so serialisation is a standard way to capture the object into a sharable format. With serialisation, we can transfer objects:

% \textbf{Over the wire} for messaging use cases

% \textbf{From application to application} via web services such as REST APIs

% \textbf{Through firewalls} (as JSON or XML strings)

% \textbf{Across domains}

% \textbf{To other data stores}

% \textbf{To identify changes in data} over time

% \textbf{While honoring security} and user-specific details across applications \\

\textbf{JavaScript Object Notation (JSON)} is... \\

...

\subsection{Data Parsing}

...

\subsection{Typesetting}

\textbf{XML} (or eXtensible Markup Language) is...

\subsection{Backwards Compatibility}

...

\subsection{Language Extensibility}

...

\end{document}
